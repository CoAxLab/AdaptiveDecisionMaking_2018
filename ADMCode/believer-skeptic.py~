from __future__ import division
import os
import numpy as np
import pandas as pd
from numpy.random import sample as rs
from copy import deepcopy


class IowaGamblingTask(object):
    """ defines a multi-armed bandit task

    ::Arguments::
        
    """
    def __init__(self, feedback, nblocks=1):

        if nblocks>1:
            feedback = feedback.append([feedback]*(nblocks-1)).reset_index()

        feedback.rename(columns={'index':'t'}, inplace=True)
        self.feedback = feedback

        self.names = np.sort(self.feedback.columns.values)
        self.ntrials=self.feedback.shape[0]

        self.choices, self.all_traces = [], []
        self.rts={k:[] for k in self.names}

        self.qdict={k:[0] for k in self.names}
        self.choice_prob={k:[1/self.names.size] for k in self.names}


    def get_feedback(self, trial, action_ix):

        choice_name = self.names[action_ix]
        return self.feedback.loc[trial, choice_name]


def play_IGT(p, feedback, alphaGo=.1, alphaNo=.1, beta=.2, nblocks=2, singleProcess=True):
    """ 
    ::Arguments::
        p (dict): parameter dictionary for accumulator
        feedback (dataframe): IGT card deck values
        alphaGo (float): learning rate for vd (direct pathway)
        alphaNo (float): learning rate for vi (indirect pathway)
        beta (float): inverse temperature parameter
        nblocks (int): number of IGT blocks to simulate
        singleProcess (bool): if true simulate accumulator with v = v_d - v_i
        
    ::Returns::
        qpDF, trialwise Q/P values for each IGT deck
        agentDF, trialwise choice, response time, and drift-rates (vd, vi)
    """

    names = np.sort(feedback.columns.values)
    nact = len(names)
    actions = np.arange(nact)
    IGT = IowaGamblingTask(feedback, nblocks=nblocks)
    ntrials=IGT.feedback.shape[0]
    
    Qmatrix = np.ones((ntrials, nact))*.05
    Pmatrix=np.zeros_like(Qmatrix)
    Qvalues = Qmatrix[0, :]
    Pvalues = np.array([1/nact]*nact)
    
    agent = np.zeros((ntrials, 3 + nact*3))
    agent[0, 3:] = np.hstack([p['vd'], p['vi'], p['vd']-p['vi']])
    
    #driftRates = np.zeros(ntrials, )
    for t in range(ntrials):
        # select bandit arm (action)
        act_i, rt, rt_i = simulate_multirace(p, singleProcess=singleProcess)
        agent[t, :3] = act_i, rt, rt_i
        
        # observe feedback
        r = IGT.get_feedback(t, act_i)
        
        # get expected value 
        Qexpected = Qvalues[act_i]
        
        # get prediction error
        RPE = r - Qexpected
        
        # get alpha for Q-value update
        alpha = alphaGo
        if RPE<0: 
            alpha = alphaNo
        
        # update expected value and store in Qvalues array
        # update v_d or v_i (depending on RPE sign)
        Qvalues[act_i] = update_Qi(Qexpected, RPE, alpha=alpha)
        
        # update action selection probabilities 
        Pvalues = update_Pall(Qvalues, beta)
        
        # store new values in output matrices
        Qmatrix[t, :] = Qvalues
        Pmatrix[t, :] = Pvalues
        
        # re-scale drift-rates by change in Softmax probability
        deltaP = Pmatrix[t] - Pmatrix[t-1]
        p = update_drift(p, deltaP, alphaGo, alphaNo)
        agent[t, 3:] = np.hstack([p['vd'], p['vi'], p['vd']-p['vi']])
        
    return make_output_df(Qmatrix, Pmatrix, agent)


